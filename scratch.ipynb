{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "from nltk.corpus import stopwords\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###import into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdf = pd.read_csv('training_set.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rawvaldf= pd.read_csv('validation_set.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###define phrases to parse\n",
    "\n",
    "####reuse and edit code used in othe project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#noun phrase\n",
    "npatterns = \"\"\"\n",
    "    NP: {<NN.*><IN>*<NN.*>+}\n",
    "    {<JJ.*>*<NN.*>+}\n",
    "    \"\"\"\n",
    "\n",
    "#{<JJ.*>*<NN.*><CC>*<NN.*>+}\n",
    "#verb phrase patterns\n",
    "vpatterns = \"\"\"\n",
    "    VP: {<VB.*><VB.*>*}\n",
    "    \"\"\"\n",
    "# {<VB.*>+<DT|IN>*<JJ.*>*<NN.*>+<R.*>*}\n",
    "#adjective phrase patterns\n",
    "apatterns = \"\"\"\n",
    "    AP: {<JJ.*><IN><NN.*>}\n",
    "    {<RB.*><JJ.*>}\n",
    "    {<JJ.*>+}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NPChunker = RegexpParser(npatterns)\n",
    "VPChunker = RegexpParser(vpatterns)\n",
    "APChunker = RegexpParser(apatterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_text(input):\n",
    "    \"\"\"Takes a list of sentences and returns words that have their parts of speech tagged\"\"\"\n",
    "    tokenized_words = [word_tokenize(sent) for sent in input]\n",
    "    tagged_words = [pos_tag(word) for word in tokenized_words]\n",
    "    return tagged_words\n",
    "\n",
    "\n",
    "def make_tree(tagged_words, typetree = 'nps'):\n",
    "    \"\"\"takes POS tagged words and returns word tree\"\"\"\n",
    "    if typetree =='nps':\n",
    "        word_tree = [NPChunker.parse(word) for word in tagged_words]\n",
    "    if typetree =='vps':\n",
    "        word_tree = [VPChunker.parse(word) for word in tagged_words]\n",
    "    if typetree == 'aps':\n",
    "        word_tree = [APChunker.parse(word) for word in tagged_words]\n",
    "    return word_tree\n",
    "\n",
    "\n",
    "def return_a_list_of_Ps(input, ptype ='nps'):\n",
    "    \"\"\"takes list of sentences, calls prepare_text, make_tree; parses it returns set of unique phrases \n",
    "    depending on the phrase type\n",
    "    \"\"\"\n",
    "    ps = []  # an empty list in which to phrases will be stored.\n",
    "    t_words = prepare_text(input)\n",
    "    p_tree = make_tree(t_words, ptype)\n",
    "#     print p_tree\n",
    "    for sent in p_tree:\n",
    "        if ptype =='nps':\n",
    "            tree = NPChunker.parse(sent)\n",
    "        if ptype =='vps':\n",
    "            tree = VPChunker.parse(sent)\n",
    "        if ptype =='aps':\n",
    "            tree = APChunker.parse(sent)\n",
    "\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == ptype.upper()[:2]:\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                ps.append(t)\n",
    "    return list(set(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rawdf['question_sent'] = rawdf['question'].map(lambda x: [x.encode('utf-8','ignore') for x in x])\n",
    "rawdf['question_sent'] = rawdf['question'].map(lambda x: sent_tokenize(x.decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawdf['qkeyps'] = rawdf['question_sent'].map(lambda x: list(set(return_a_list_of_Ps(x)+return_a_list_of_Ps(x,'vps')+return_a_list_of_Ps(x,'aps'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(inputlist):\n",
    "    t_copy = word_tokenize(u' '.join(inputlist))\n",
    "    dups = list(set([x for x in t_copy if t_copy.count(x) > 1]))\n",
    "    for i in dups:\n",
    "        while i in inputlist:\n",
    "            inputlist.remove(i)\n",
    "    return inputlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdf['u_qkeyps'] = rawdf['qkeyps'].map(lambda x: flatten(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 105\n",
    "print rawdf.u_qkeyps[i]\n",
    "print rawdf.question[i]\n",
    "rawdf[i:i+1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
